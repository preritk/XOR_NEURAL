{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import scatter, show, legend, xlabel, ylabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LAYER = 2    #NO. of nodes in input layer\n",
    "HIDDEN_LAYER = 3   #NO. of nodes in hidden layer\n",
    "OUTPUT_LAYER = 1   #NO. of nodes in otput layer\n",
    "learning_rate = .8 #Learning rate\n",
    "lamda = 0.0        #Regularisation not taken in consideration\n",
    "epochs = 128    #Number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayData(X,Y):             #Displays data\n",
    "    for i in range(len(X)):\n",
    "        if(Y[i]==1):           # 1 is marked by o\n",
    "            plt.scatter(X[i][0], X[i][1],marker = 'o',color = 'b',s = 30)\n",
    "        else:                  # 0 is marked by x\n",
    "            plt.scatter(X[i][0],X[i][1],marker = 'x' , color = 'r',s = 30)\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.legend(['1','0'])\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Randomise():\n",
    "    #Here the value of each element of Theta_1,Theta_2 lies between [-Epsilon_init,+Epsilon_init]\n",
    "    EPSILON_INIT = 0.12\n",
    "    Theta_1 = np.random.rand(HIDDEN_LAYER,INPUT_LAYER+1)*(2*EPSILON_INIT)-(EPSILON_INIT)     # Random Weights for level 1\n",
    "    Theta_2 = np.random.rand(OUTPUT_LAYER,HIDDEN_LAYER+1)*(2*EPSILON_INIT)-(EPSILON_INIT)    # Random Weights for level 2\n",
    "    return Theta_1,Theta_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    t = 1.0/(1.0 + np.exp(-1.0 * z))       #Sigmoid function\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    p = sigmoid(z)\n",
    "    q = 1 - p\n",
    "    return np.multiply(p,q)                #returns the derivative of sigmoid value for any z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardProp(a1,Theta_1,Theta_2):\n",
    "    a1 = [[1],[a1[0]],[a1[1]]]             # bias value added to features\n",
    "    z2 = np.matmul(Theta_1,a1)\n",
    "    a2 = sigmoid(z2)                       #Activation values for layer 2\n",
    "    a2 = [[1],[a2[0]],[a2[1]],[a2[2]]]     #Bias value added to layer 2 actvation values\n",
    "    z3 = np.matmul(Theta_2,a2)\n",
    "    h = sigmoid(z3)                        #Activation value for layer 3\n",
    "    h = [h[0]]                             #Just converting to numpy array\n",
    "    return h,z3,a2,a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costfunction_NN(X,Y,Theta_1,Theta_2):\n",
    "    H = []\n",
    "    for i in range(len(X)):\n",
    "        h,z3,a2,a1 = ForwardProp(X[i],Theta_1,Theta_2) #z3,a2,a1 are useless here.They are included to maintain the pattern\n",
    "        H = H + h\n",
    "    Y = [[Y[0]],[Y[1]],[Y[2]],[Y[3]]]\n",
    "#     J = 0.0\n",
    "    J = (-1.0/4)*(np.matmul(np.transpose(Y),np.log(H)) + np.matmul(np.transpose(np.ones((4,1),dtype = float)-Y),np.log(np.ones((4,1),dtype = float)-H)))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_NN(X,Y,Theta_1,Theta_2):\n",
    "    x1 = []\n",
    "    cost1 = []\n",
    "    Theta1_grad = np.zeros(np.shape(Theta_1))  # Defining initial gradient matrices for Theta_1 and Theta_2\n",
    "    Theta2_grad = np.zeros(np.shape(Theta_2))\n",
    "    for x in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            h,z3,a2,a1 = ForwardProp(X[i],Theta_1,Theta_2)  #Forward Propogation for eacg training example\n",
    "            \"\"\"Backward Propogation\"\"\"\n",
    "            delta_3 = h[0] - Y[i]                  #Error in layer 3\n",
    "            delta_3 = [[delta_3[0]]]                  #Just converting to 1x1 matrix\n",
    "            delta_2 = np.multiply(np.matmul(np.transpose(Theta_2),delta_3),sigmoidGradient(z3))\n",
    "            delta_2 = [delta_2[1],delta_2[2],delta_2[3]]     #Error in layer 2\n",
    "\n",
    "            #No delta_1 defined because it is feature matrix which is errorless\n",
    "\n",
    "            Theta2_grad = np.divide(Theta2_grad - np.matmul(delta_3,np.transpose(a2)),4)\n",
    "            Theta1_grad = np.divide(Theta1_grad - np.matmul(delta_2,np.transpose(a1)),4)\n",
    "\n",
    "        Theta_1 = Theta_1 + ((learning_rate)*(Theta1_grad))     #Updating Theta_1\n",
    "        Theta_2 = Theta_2 + ((learning_rate)*(Theta2_grad))     #Updating Theta_2\n",
    "        cost = costfunction_NN(X,Y,Theta_1,Theta_2)             #Calculating cost\n",
    "        # plt.scatter(cost,x,marker = 'x',color = 'r')\n",
    "#         print('x:', x)\n",
    "#         print('cost:', cost[0][0])\n",
    "        x1.append([x])\n",
    "        cost1.append(cost[0][0])\n",
    "#     plt.plot(x1, cost1, '-')\n",
    "#     plt.xlabel('No. of iterations')\n",
    "#     plt.ylabel('Cost')\n",
    "#     plt.show()\n",
    "#     print(np.min(x1), np.min(cost1), \"\\r\", end=\"\")\n",
    "    return Theta_1,Theta_2, np.min(cost1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "with open(\"XOR.csv\",\"r\") as csvfile:      #importing data from csv file\n",
    "    lines = csv.reader(csvfile)\n",
    "    dataset = list(lines)\n",
    "    for i in range(len(dataset)):\n",
    "        X.append([int(dataset[i][0]),int(dataset[i][1])])      #Feature matrix formation\n",
    "        Y.append(int(dataset[i][2]))                           #Output matrix formation\n",
    "displayData(X,Y)                                               #Displaying the data\n",
    "Theta_1,Theta_2 = Randomise()                                  #Randomising Theta_1,Theta_2\n",
    "curr = 10\n",
    "th = .51\n",
    "while curr > th:\n",
    "    Theta_1,Theta_2, curr = Train_NN(X,Y,Theta_1,Theta_2)                 #Training Neural Network\n",
    "    print(\"\\r\", curr, end=\"\")\n",
    "h1, z3, a2, a1 = ForwardProp(X[0], Theta_1, Theta_2)\n",
    "h2, z3, a2, a1 = ForwardProp(X[1], Theta_1, Theta_2)\n",
    "h3, z3, a2, a1 = ForwardProp(X[2], Theta_1, Theta_2)\n",
    "h4, z3, a2, a1 = ForwardProp(X[3], Theta_1, Theta_2)\n",
    "print(h1,h2,h3,h4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Theta_1, Theta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
